package com.bigdatacompany.eticaret.consumer;

import com.mongodb.client.MongoClient;
import com.mongodb.client.MongoClients;
import com.mongodb.client.MongoCollection;
import com.mongodb.client.MongoDatabase;
import org.apache.spark.sql.Dataset;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SparkSession;
import org.apache.spark.sql.functions;
import org.apache.spark.sql.types.DataTypes;
import org.apache.spark.sql.types.StructType;
import org.bson.Document;

import java.time.Instant;
import java.time.LocalDateTime;
import java.time.format.DateTimeFormatter;
import java.util.ArrayList;
import java.util.List;

import static org.apache.spark.sql.functions.*;

public class SparkConsumerApplication {

    private static final String KAFKA_BOOTSTRAP_SERVERS = "localhost:9092";
    private static final String KAFKA_TOPIC = "search-analysisv2";

    // MongoDB ayarlarÄ±
    private static final String MONGODB_URI = "mongodb://admin:admin123@localhost:27017/?authSource=admin";
    private static final String MONGODB_DATABASE = "eticaret_analytics";

    // MongoDB client (singleton)
    private static MongoClient mongoClient;
    private static MongoDatabase mongoDatabase;

    public static void main(String[] args) {
        // Windows iÃ§in Hadoop home directory ayarÄ±
        String hadoopHome = System.getenv("HADOOP_HOME");
        if (hadoopHome == null || hadoopHome.isEmpty()) {
            hadoopHome = "C:\\hadoop";
        }
        System.setProperty("hadoop.home.dir", hadoopHome);

        // MongoDB baÄŸlantÄ±sÄ±nÄ± kur
        try {
            mongoClient = MongoClients.create(MONGODB_URI);
            mongoDatabase = mongoClient.getDatabase(MONGODB_DATABASE);
            System.out.println("âœ… MongoDB baÄŸlantÄ±sÄ± kuruldu.");
        } catch (Exception e) {
            System.err.println("âŒ MongoDB baÄŸlantÄ± hatasÄ±: " + e.getMessage());
            return;
        }

        // JSON schema tanÄ±mÄ±
        StructType schema = new StructType()
                .add("search", DataTypes.StringType)
                .add("region", DataTypes.StringType)
                .add("current_ts", DataTypes.StringType)
                .add("timestamp", DataTypes.StringType);

        // Spark Session oluÅŸtur
        SparkSession sparkSession = SparkSession.builder()
                .master("local[*]")
                .appName("E-Ticaret Arama Analizi")
                .config("spark.sql.streaming.schemaInference", "true")
                .config("spark.driver.host", "localhost")
                .config("spark.ui.enabled", "false")
                .getOrCreate();

        // Log seviyesini ayarla
        sparkSession.sparkContext().setLogLevel("WARN");

        System.out.println("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
        System.out.println("â•‘     E-Ticaret Arama Analizi - Spark Streaming + MongoDB      â•‘");
        System.out.println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
        System.out.println("â•‘  Kafka: " + String.format("%-52s", KAFKA_BOOTSTRAP_SERVERS) + "â•‘");
        System.out.println("â•‘  Topic: " + String.format("%-52s", KAFKA_TOPIC) + "â•‘");
        System.out.println("â•‘  MongoDB: " + String.format("%-50s", "localhost:27017/" + MONGODB_DATABASE) + "â•‘");
        System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

        try {
            // Kafka'dan veri oku
            Dataset<Row> kafkaDF = sparkSession
                    .readStream()
                    .format("kafka")
                    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
                    .option("subscribe", KAFKA_TOPIC)
                    .option("startingOffsets", "latest")
                    .option("failOnDataLoss", "false")
                    .load();

            // JSON parse et
            Dataset<Row> parsedDF = kafkaDF
                    .selectExpr("CAST(value AS STRING) as json_value")
                    .select(functions.from_json(col("json_value"), schema).as("data"))
                    .select("data.search", "data.region", "data.current_ts", "data.timestamp");

            // Arama istatistikleri
            Dataset<Row> searchCounts = parsedDF
                    .filter(col("search").isNotNull())
                    .groupBy("search")
                    .count();

            // BÃ¶lge istatistikleri
            Dataset<Row> regionCounts = parsedDF
                    .filter(col("search").isNotNull())
                    .groupBy("region")
                    .count();

            // Zaman bazlÄ± istatistikler - Saatlik ve gÃ¼nlÃ¼k gruplama
            Dataset<Row> timeStats = parsedDF
                    .filter(col("search").isNotNull())
                    .withColumn("event_hour", hour(current_timestamp()))
                    .withColumn("event_date", date_format(current_timestamp(), "yyyy-MM-dd"))
                    .groupBy("event_hour", "event_date")
                    .count();

            // Arama istatistiklerini iÅŸle
            searchCounts
                    .writeStream()
                    .outputMode("complete")
                    .foreachBatch((batchDF, batchId) -> {
                        if (batchDF.count() > 0) {
                            System.out.println("\nâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—");
                            System.out.println(
                                    "â•‘  ARAMA Ä°STATÄ°STÄ°KLERÄ° - Batch: " + String.format("%-9d", batchId) + "â•‘");
                            System.out.println("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£");
                            System.out.println(
                                    "â•‘  Toplam farklÄ± arama: " + String.format("%-18d", batchDF.count()) + "â•‘");
                            System.out.println("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•");

                            System.out.println("\nğŸ“Š En Ã‡ok Aranan ÃœrÃ¼nler:");
                            batchDF.orderBy(functions.desc("count")).show(10, false);

                            // MongoDB'ye yaz
                            writeToMongoDB(batchDF, "search_stats", "search", batchId);
                        }
                    })
                    .trigger(org.apache.spark.sql.streaming.Trigger.ProcessingTime("5 seconds"))
                    .start();

            // BÃ¶lge istatistiklerini iÅŸle
            regionCounts
                    .writeStream()
                    .outputMode("complete")
                    .foreachBatch((batchDF, batchId) -> {
                        if (batchDF.count() > 0) {
                            System.out.println("\nğŸ—ºï¸ BÃ¶lgelere GÃ¶re Arama DaÄŸÄ±lÄ±mÄ±:");
                            batchDF.orderBy(functions.desc("count")).show(10, false);

                            // MongoDB'ye yaz
                            writeToMongoDB(batchDF, "region_stats", "region", batchId);
                        }
                    })
                    .trigger(org.apache.spark.sql.streaming.Trigger.ProcessingTime("5 seconds"))
                    .start();

            // Zaman istatistiklerini iÅŸle
            timeStats
                    .writeStream()
                    .outputMode("complete")
                    .foreachBatch((batchDF, batchId) -> {
                        if (batchDF.count() > 0) {
                            System.out.println("\nâ° Zaman BazlÄ± Ä°statistikler:");
                            batchDF.orderBy(col("event_date").desc(), col("event_hour").desc()).show(24, false);

                            // MongoDB'ye yaz
                            writeTimeStatsToMongoDB(batchDF, batchId);
                        }
                    })
                    .trigger(org.apache.spark.sql.streaming.Trigger.ProcessingTime("5 seconds"))
                    .start();

            // TÃ¼m stream'lerin bitmesini bekle
            sparkSession.streams().awaitAnyTermination();

        } catch (Exception e) {
            System.err.println("Streaming hatasÄ±: " + e.getMessage());
            e.printStackTrace();
        } finally {
            if (mongoClient != null) {
                mongoClient.close();
            }
            sparkSession.stop();
        }
    }

    /**
     * DataFrame verilerini MongoDB'ye yazar
     * Her batch'te koleksiyonu temizleyip gÃ¼ncel verileri yazar
     */
    private static void writeToMongoDB(Dataset<Row> df, String collectionName, String keyField, long batchId) {
        try {
            MongoCollection<Document> collection = mongoDatabase.getCollection(collectionName);

            List<Row> rows = df.collectAsList();
            List<Document> documents = new ArrayList<>();

            for (Row row : rows) {
                Document doc = new Document();
                doc.append(keyField, row.getString(0));
                doc.append("count", row.getLong(1));
                doc.append("batch_id", batchId);
                doc.append("updated_at", Instant.now().toString());
                documents.add(doc);
            }

            if (!documents.isEmpty()) {
                // TÃœM eski verileri sil ve gÃ¼ncel verileri ekle (tutarlÄ±lÄ±k iÃ§in)
                collection.deleteMany(new Document());
                collection.insertMany(documents);
                System.out.println("âœ… " + collectionName + " MongoDB'ye kaydedildi. (" + documents.size() + " kayÄ±t)");
            }
        } catch (Exception e) {
            System.err.println("âš ï¸ MongoDB yazma hatasÄ± (" + collectionName + "): " + e.getMessage());
            e.printStackTrace();
        }
    }

    /**
     * Zaman bazlÄ± istatistikleri MongoDB'ye yazar
     * Saatlik ve gÃ¼nlÃ¼k verileri time_stats koleksiyonuna kaydeder
     */
    private static void writeTimeStatsToMongoDB(Dataset<Row> df, long batchId) {
        try {
            MongoCollection<Document> collection = mongoDatabase.getCollection("time_stats");

            List<Row> rows = df.collectAsList();
            List<Document> documents = new ArrayList<>();

            for (Row row : rows) {
                Document doc = new Document();
                doc.append("hour", row.getInt(0));           // event_hour
                doc.append("date", row.getString(1));         // event_date
                doc.append("count", row.getLong(2));          // count
                doc.append("batch_id", batchId);
                doc.append("updated_at", Instant.now().toString());
                documents.add(doc);
            }

            if (!documents.isEmpty()) {
                // TÃœM eski verileri sil ve gÃ¼ncel verileri ekle
                collection.deleteMany(new Document());
                collection.insertMany(documents);
                System.out.println("âœ… time_stats MongoDB'ye kaydedildi. (" + documents.size() + " kayÄ±t)");
            }
        } catch (Exception e) {
            System.err.println("âš ï¸ MongoDB yazma hatasÄ± (time_stats): " + e.getMessage());
            e.printStackTrace();
        }
    }
}
